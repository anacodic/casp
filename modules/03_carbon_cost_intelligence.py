"""
MODULE 03: CARBON COST OF INTELLIGENCE
======================================
Calculates AI compute carbon based on:
1. Model type (GPT-4, Gemini, local GradientBoosting)
2. Grid carbon intensity (country/region)
3. Number of inferences

This module bridges the Energy paper (Kaur et al. 2026) to supply chain optimization.
It quantifies the computational carbon footprint of AI-enabled decision support.

Key Question Answered: "Is using AI worth the carbon it adds?"

References:
- Kaur et al. (2026) "The Carbon Cost of Intelligence" - Energies
- Patterson et al. (2021) "Carbon emissions and large neural network training"
- Strubell et al. (2019) "Energy and Policy Considerations for Deep Learning in NLP"
"""

from typing import Dict, Optional
from dataclasses import dataclass
from config.grid_carbon import GRID_CARBON_INTENSITY

@dataclass
class AIModelConfig:
    """Configuration for an AI model's energy characteristics."""
    name: str
    energy_per_inference_wh: float  # Watt-hours per inference
    description: str
    source: str


class CarbonCostOfIntelligence:
    """
    Calculates the carbon footprint of AI inference operations.
    
    This addresses the research gap: existing literature applies AI to reduce 
    transportation emissions but does not quantify the computational carbon 
    footprint of the AI systems themselves.
    """
    
    # Energy per inference (Wh) - based on literature and benchmarks
    # Sources: Patterson et al. (2021), Kaur et al. (2026)
    MODEL_ENERGY_WH = {
        # Large Language Models (AWS Bedrock - Claude family)
        'claude-3-haiku': 0.0006,     # ~0.6 Wh per request (lightweight, fast)
        'claude-3-sonnet': 0.0015,    # ~1.5 Wh per request (balanced, production)
        'claude-3-opus': 0.0027,      # ~2.7 Wh per request (high-performance)
        # Legacy models (kept for reference, not used in Bedrock)
        'gpt-4': 0.0029,              # ~2.9 Wh per request (OpenAI, not in Bedrock)
        'gpt-4-turbo': 0.0023,        # ~2.3 Wh per request (OpenAI, not in Bedrock)
        'gpt-3.5-turbo': 0.0004,      # ~0.4 Wh per request (OpenAI, not in Bedrock)
        'gemini-pro': 0.0018,         # ~1.8 Wh per request (Google, not in Bedrock)
        'gemini-flash': 0.0006,       # ~0.6 Wh per request (Google, not in Bedrock)
        
        # Local ML Models (much smaller)
        'gradient-boosting': 0.00001,  # ~0.01 Wh (negligible)
        'random-forest': 0.00001,      # ~0.01 Wh
        'xgboost': 0.00002,            # ~0.02 Wh
        'neural-net-small': 0.0001,    # ~0.1 Wh (small MLP)
        
        # Default for unknown models
        'default': 0.001               # ~1 Wh (conservative estimate)
    }
    
    # Tokens per typical supply chain query (for LLM-based optimization)
    TOKENS_PER_QUERY = {
        'route_optimization': 1500,    # Complex reasoning about routes
        'demand_forecasting': 800,     # Time series analysis
        'vendor_selection': 600,       # Comparison task
        'risk_assessment': 1000,       # Multi-factor analysis
        'simple_query': 200            # Basic lookups
    }
    
    def __init__(self, default_country: str = 'usa'):
        """
        Initialize the carbon calculator.
        
        Args:
            default_country: Default country for grid carbon intensity
        """
        self.default_country = default_country.lower()
        self.total_inferences = 0
        self.total_carbon_gco2 = 0.0
        self.inference_log = []
    
    def get_grid_intensity(self, country: str) -> float:
        """
        Get grid carbon intensity for a country.
        
        Args:
            country: Country name
            
        Returns:
            Carbon intensity in gCO2/kWh
        """
        return GRID_CARBON_INTENSITY.get(country.lower(), 400)  # Default 400 if unknown
    
    def calculate_inference_carbon(
        self,
        model_type: str,
        country: str = None,
        num_inferences: int = 1,
        task_type: str = 'default'
    ) -> Dict:
        """
        Calculate carbon footprint for AI inference(s).
        
        Args:
            model_type: Type of AI model (e.g., 'gpt-4', 'gradient-boosting')
            country: Country where compute runs (affects grid intensity)
            num_inferences: Number of inferences to calculate
            task_type: Type of task for LLM token estimation
            
        Returns:
            Dictionary with carbon breakdown
        """
        if country is None:
            country = self.default_country
        
        # Get energy per inference
        energy_wh = self.MODEL_ENERGY_WH.get(model_type.lower(), self.MODEL_ENERGY_WH['default'])
        
        # Get grid carbon intensity
        grid_intensity = self.get_grid_intensity(country)
        
        # Calculate carbon: energy (kWh) Ã— grid intensity (gCO2/kWh)
        energy_kwh = (energy_wh * num_inferences) / 1000
        carbon_gco2 = energy_kwh * grid_intensity
        
        # Track cumulative
        self.total_inferences += num_inferences
        self.total_carbon_gco2 += carbon_gco2
        
        result = {
            'model': model_type,
            'country': country,
            'num_inferences': num_inferences,
            'energy_per_inference_wh': energy_wh,
            'total_energy_wh': energy_wh * num_inferences,
            'total_energy_kwh': energy_kwh,
            'grid_intensity_gco2_kwh': grid_intensity,
            'carbon_gco2': carbon_gco2,
            'carbon_kg': carbon_gco2 / 1000
        }
        
        # Log for auditing
        self.inference_log.append(result)
        
        return result
    
    def calculate_optimization_carbon(
        self,
        num_routes: int,
        model_type: str = 'gradient-boosting',
        country: str = None,
        include_llm_reasoning: bool = False
    ) -> Dict:
        """
        Calculate total carbon for a supply chain optimization task.
        
        This accounts for:
        1. ML model predictions (cost, carbon, on-time for each route)
        2. LLM reasoning (3 calls: Orchestrator + Risk Agent + Sourcing Agent)
        
        Args:
            num_routes: Number of routes to evaluate
            model_type: ML model type for predictions
            country: Country for compute
            include_llm_reasoning: Whether to include 3 LLM calls (REQUIRED for multi-agent system)
            
        Returns:
            Complete carbon breakdown
        """
        if country is None:
            country = self.default_country
        
        # ML predictions: 3 predictions per route (cost, carbon, on-time)
        ml_inferences = num_routes * 3
        ml_carbon = self.calculate_inference_carbon(
            model_type=model_type,
            country=country,
            num_inferences=ml_inferences,
            task_type='route_optimization'
        )
        
        result = {
            'task': 'route_optimization',
            'num_routes_evaluated': num_routes,
            'ml_model': model_type,
            'ml_inferences': ml_inferences,
            'ml_carbon_gco2': ml_carbon['carbon_gco2'],
            'country': country,
            'grid_intensity': ml_carbon['grid_intensity_gco2_kwh']
        }
        
        # Add LLM reasoning if used (3 LLM calls: Orchestrator + Risk Agent + Sourcing Agent)
        if include_llm_reasoning:
            # Default to Claude-3-Sonnet (baseline model used in production)
            llm_model = 'claude-3-sonnet'
            llm_carbon = self.calculate_inference_carbon(
                model_type=llm_model,
                country=country,
                num_inferences=3,  # 3 LLM calls per optimization
                task_type='route_optimization'
            )
            result['llm_model'] = llm_model
            result['llm_carbon_gco2'] = llm_carbon['carbon_gco2']
            result['llm_inferences'] = 3
            result['total_ai_carbon_gco2'] = ml_carbon['carbon_gco2'] + llm_carbon['carbon_gco2']
        else:
            result['total_ai_carbon_gco2'] = ml_carbon['carbon_gco2']
            result['llm_model'] = None
            result['llm_carbon_gco2'] = 0.0
            result['llm_inferences'] = 0
        
        return result
    
    def compare_ai_vs_no_ai(
        self,
        transport_carbon_without_ai: float,
        transport_carbon_with_ai: float,
        ai_carbon: float
    ) -> Dict:
        """
        Evaluate whether AI optimization is worth the carbon cost.
        
        This answers the key research question: Does using AI for route 
        optimization result in net carbon savings?
        
        Args:
            transport_carbon_without_ai: Transport carbon without AI optimization (gCO2)
            transport_carbon_with_ai: Transport carbon with AI-selected route (gCO2)
            ai_carbon: Carbon from AI computation (gCO2)
            
        Returns:
            Analysis of AI carbon ROI
        """
        transport_saved = transport_carbon_without_ai - transport_carbon_with_ai
        total_with_ai = transport_carbon_with_ai + ai_carbon
        net_savings = transport_carbon_without_ai - total_with_ai
        
        # Carbon ROI: how much transport carbon saved per unit of AI carbon
        carbon_roi = transport_saved / ai_carbon if ai_carbon > 0 else float('inf')
        
        return {
            'baseline_transport_gco2': transport_carbon_without_ai,
            'optimized_transport_gco2': transport_carbon_with_ai,
            'transport_savings_gco2': transport_saved,
            'ai_compute_carbon_gco2': ai_carbon,
            'total_with_ai_gco2': total_with_ai,
            'net_carbon_savings_gco2': net_savings,
            'carbon_roi': carbon_roi,
            'ai_worth_it': net_savings > 0,
            'savings_percentage': (net_savings / transport_carbon_without_ai) * 100 if transport_carbon_without_ai > 0 else 0,
            'interpretation': self._interpret_roi(carbon_roi, net_savings)
        }
    
    def _interpret_roi(self, roi: float, net_savings: float) -> str:
        """Generate human-readable interpretation of carbon ROI."""
        if net_savings < 0:
            return f"âš ï¸ AI ADDS more carbon than it saves. Net increase: {abs(net_savings):.1f} gCO2"
        elif roi > 100:
            return f"âœ… Excellent ROI: AI saves {roi:.0f}Ã— its own carbon cost"
        elif roi > 10:
            return f"âœ… Good ROI: AI saves {roi:.0f}Ã— its own carbon cost"
        elif roi > 1:
            return f"âœ… Positive ROI: AI saves {roi:.1f}Ã— its own carbon cost"
        else:
            return "âš ï¸ Marginal benefit: AI barely breaks even on carbon"
    
    def compare_countries(
        self,
        model_type: str,
        num_inferences: int = 1,
        countries: list = None
    ) -> Dict:
        """
        Compare AI carbon footprint across different countries.
        
        Demonstrates how grid carbon intensity affects AI sustainability.
        
        Args:
            model_type: AI model to evaluate
            num_inferences: Number of inferences
            countries: List of countries to compare
            
        Returns:
            Comparison across countries
        """
        if countries is None:
            countries = ['india', 'china', 'usa', 'germany', 'uk', 'france', 'norway']
        
        results = {}
        for country in countries:
            carbon = self.calculate_inference_carbon(
                model_type=model_type,
                country=country,
                num_inferences=num_inferences
            )
            results[country] = {
                'grid_intensity': carbon['grid_intensity_gco2_kwh'],
                'carbon_gco2': carbon['carbon_gco2']
            }
        
        # Find min/max
        sorted_countries = sorted(results.items(), key=lambda x: x[1]['carbon_gco2'])
        cleanest = sorted_countries[0]
        dirtiest = sorted_countries[-1]
        
        return {
            'model': model_type,
            'num_inferences': num_inferences,
            'by_country': results,
            'cleanest': {'country': cleanest[0], **cleanest[1]},
            'dirtiest': {'country': dirtiest[0], **dirtiest[1]},
            'ratio': dirtiest[1]['carbon_gco2'] / cleanest[1]['carbon_gco2'] if cleanest[1]['carbon_gco2'] > 0 else float('inf')
        }
    
    def get_cumulative_stats(self) -> Dict:
        """Get cumulative carbon statistics for all tracked inferences."""
        return {
            'total_inferences': self.total_inferences,
            'total_carbon_gco2': self.total_carbon_gco2,
            'total_carbon_kg': self.total_carbon_gco2 / 1000,
            'avg_carbon_per_inference_gco2': self.total_carbon_gco2 / self.total_inferences if self.total_inferences > 0 else 0
        }
    
    def reset_tracking(self):
        """Reset cumulative tracking."""
        self.total_inferences = 0
        self.total_carbon_gco2 = 0.0
        self.inference_log = []


def calculate_ai_carbon_for_route_optimization(
    num_routes: int,
    country: str = 'usa',
    use_llm: bool = False
) -> float:
    """
    Convenience function to get AI carbon for route optimization.
    
    This replaces the hardcoded 50 gCO2 value.
    
    Args:
        num_routes: Number of routes being evaluated
        country: Country where compute runs
        use_llm: Whether LLM is used for reasoning
        
    Returns:
        Total AI carbon in gCO2
    """
    cci = CarbonCostOfIntelligence(default_country=country)
    result = cci.calculate_optimization_carbon(
        num_routes=num_routes,
        model_type='gradient-boosting',
        country=country,
        include_llm_reasoning=use_llm
    )
    return result['total_ai_carbon_gco2']


if __name__ == "__main__":
    print("=" * 70)
    print("MODULE 03: CARBON COST OF INTELLIGENCE - TEST")
    print("=" * 70)
    
    cci = CarbonCostOfIntelligence(default_country='usa')
    
    # Test 1: Basic inference carbon calculation
    print("\nðŸ“Š TEST 1: Single Inference Carbon")
    print("-" * 40)
    
    models_to_test = ['gpt-4', 'gpt-3.5-turbo', 'gradient-boosting']
    for model in models_to_test:
        result = cci.calculate_inference_carbon(model_type=model, num_inferences=1)
        print(f"{model:20s}: {result['carbon_gco2']:.6f} gCO2")
    
    # Test 2: Country comparison
    print("\nðŸ“Š TEST 2: Country Comparison (GPT-4, 100 inferences)")
    print("-" * 40)
    
    comparison = cci.compare_countries('gpt-4', num_inferences=100)
    for country, data in comparison['by_country'].items():
        print(f"{country:10s}: {data['carbon_gco2']:.4f} gCO2 (grid: {data['grid_intensity']} gCO2/kWh)")
    
    print(f"\nâ†’ {comparison['dirtiest']['country']} produces {comparison['ratio']:.1f}Ã— more carbon than {comparison['cleanest']['country']}")
    
    # Test 3: Route optimization carbon
    print("\nðŸ“Š TEST 3: Route Optimization Carbon")
    print("-" * 40)
    
    opt_result = cci.calculate_optimization_carbon(
        num_routes=3,
        model_type='gradient-boosting',
        country='usa',
        include_llm_reasoning=False
    )
    print(f"ML-only optimization (3 routes):")
    print(f"  â€¢ ML inferences: {opt_result['ml_inferences']}")
    print(f"  â€¢ AI carbon: {opt_result['total_ai_carbon_gco2']:.6f} gCO2")
    
    opt_with_llm = cci.calculate_optimization_carbon(
        num_routes=3,
        model_type='gradient-boosting',
        country='usa',
        include_llm_reasoning=True
    )
    print(f"\nML + LLM optimization (3 routes):")
    print(f"  â€¢ Total AI carbon: {opt_with_llm['total_ai_carbon_gco2']:.6f} gCO2")
    
    # Test 4: AI vs No AI comparison
    print("\nðŸ“Š TEST 4: Is AI Worth It?")
    print("-" * 40)
    
    # Scenario: Without AI, user picks random route (150,000 gCO2)
    # With AI, system picks optimized route (120,000 gCO2)
    # AI computation costs 0.001 gCO2
    
    worth_it = cci.compare_ai_vs_no_ai(
        transport_carbon_without_ai=150000,  # Random route
        transport_carbon_with_ai=120000,     # AI-optimized route
        ai_carbon=0.001                      # AI compute cost
    )
    
    print(f"Baseline transport:     {worth_it['baseline_transport_gco2']:,.0f} gCO2")
    print(f"Optimized transport:    {worth_it['optimized_transport_gco2']:,.0f} gCO2")
    print(f"AI compute carbon:      {worth_it['ai_compute_carbon_gco2']:.6f} gCO2")
    print(f"Net savings:            {worth_it['net_carbon_savings_gco2']:,.0f} gCO2 ({worth_it['savings_percentage']:.1f}%)")
    print(f"Carbon ROI:             {worth_it['carbon_roi']:,.0f}Ã—")
    print(f"\n{worth_it['interpretation']}")
    
    # Test 5: Key finding - AI carbon is negligible
    print("\nðŸ“Š TEST 5: KEY RESEARCH FINDING")
    print("-" * 40)
    
    transport_carbon = 150000  # gCO2 for typical route
    ai_carbon = opt_result['total_ai_carbon_gco2']
    ratio = ai_carbon / transport_carbon * 100
    
    print(f"Transport carbon:  {transport_carbon:,} gCO2")
    print(f"AI carbon:         {ai_carbon:.6f} gCO2")
    print(f"AI as % of total:  {ratio:.8f}%")
    print(f"\nâ†’ AI carbon is {transport_carbon/ai_carbon:,.0f}Ã— smaller than transport carbon")
    print("â†’ The 'carbon cost of intelligence' is negligible for local ML models")
    print("â†’ Even GPT-4 adds only ~0.001% to total supply chain carbon")
    
    print("\n" + "=" * 70)
    print("âœ… Module 03 tests complete!")
    print("=" * 70)
