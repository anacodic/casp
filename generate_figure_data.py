"""
Generate CSV/data for paper figures. Run from code/ directory.
Writes to ../MDPI_Systems_Paper/figdata/
"""

import os
import sys
import csv
import json
import time

script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)
sys.path.insert(0, script_dir)

import importlib
_mod01 = importlib.import_module("modules.01_predictive_analytics")
_mod02 = importlib.import_module("modules.02_vendor_segmentation")
_mod07 = importlib.import_module("modules.07_early_warning_system")
_mod03 = importlib.import_module("modules.03_carbon_cost_intelligence")
CarbonCostOfIntelligence = _mod03.CarbonCostOfIntelligence

out_dir = os.path.join(script_dir, "..", "MDPI_Systems_Paper", "figdata")
os.makedirs(out_dir, exist_ok=True)

def _write_csv(path, rows, header):
    with open(path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(header)
        w.writerows(rows)

def feature_importance_delay():
    """Module 07: top 10 feature importances for delay prediction."""
    EWS = _mod07.EarlyWarningSystem
    ews = EWS()
    ews.load_data()
    ews.train_delay_predictor()
    # Preprocessor is already fitted in train_delay_predictor
    names = ews.preprocessor.get_feature_names_out()
    imp = ews.delay_model.feature_importances_
    # Shorten names for display (remove prefix)
    short = [n.replace("num__", "").replace("cat__", "")[:25] for n in names]
    pairs = sorted(zip(short, (float(i) for i in imp)), key=lambda x: -x[1])[:10]
    _write_csv(os.path.join(out_dir, "feature_importance_delay.csv"), [(p[0], p[1]) for p in pairs], ["feature", "importance"])

def feature_importance_ontime():
    """Module 01: top 10 feature importances for on-time prediction."""
    PA = _mod01.PredictiveAnalytics
    pa = PA()
    pa.load_data()
    pa.prepare_features()
    pa.train_models()
    names = pa.preprocessor.get_feature_names_out()
    imp = pa.on_time_model.feature_importances_
    short = [n.replace("num__", "").replace("cat__", "")[:25] for n in names]
    pairs = sorted(zip(short, (float(i) for i in imp)), key=lambda x: -x[1])[:10]
    _write_csv(os.path.join(out_dir, "feature_importance_ontime.csv"), [(p[0], p[1]) for p in pairs], ["feature", "importance"])

def forecast_failure_mae():
    """MAE by weather_condition and by delivery_partner for on-time prediction (Module 01)."""
    PA = _mod01.PredictiveAnalytics
    pa = PA()
    pa.load_data()
    pa.prepare_features()
    pa.train_models()
    from sklearn.model_selection import train_test_split
    X = pa.prepare_features()
    y = pa.df["on_time_pct"].values
    X_train, X_test, y_train, y_test = train_test_split(pa.df.drop(columns=pa.df.columns.difference(
        ['delivery_partner', 'package_type', 'vehicle_type', 'delivery_mode', 'region', 'weather_condition', 'distance_km', 'package_weight_kg', 'delivery_rating'])),
        y, test_size=0.2, random_state=42)
    # We need test set indices to group by weather/partner
    _, test_idx = train_test_split(pa.df.index, test_size=0.2, random_state=42)
    test_df = pa.df.loc[test_idx].copy()
    X_test = pa.preprocessor.transform(test_df[['delivery_partner', 'package_type', 'vehicle_type', 'delivery_mode', 'region', 'weather_condition', 'distance_km', 'package_weight_kg', 'delivery_rating']])
    y_test = pa.df.loc[test_idx, "on_time_pct"].values
    pred = pa.on_time_model.predict(X_test)
    test_df["pred"] = pred
    test_df["actual"] = y_test
    test_df["ae"] = abs(pred - y_test)
    by_weather = test_df.groupby("weather_condition")["ae"].mean().reset_index()
    by_weather.columns = ["category", "mae"]
    by_weather["group"] = "weather"
    by_partner = test_df.groupby("delivery_partner")["ae"].mean().reset_index()
    by_partner.columns = ["category", "mae"]
    by_partner["group"] = "partner"
    rows = [["category", "mae", "group"]]
    for _, r in by_weather.iterrows():
        rows.append([r["category"], round(r["mae"], 2), "weather"])
    for _, r in by_partner.iterrows():
        rows.append([r["category"][:20], round(r["mae"], 2), "partner"])
    _write_csv(os.path.join(out_dir, "forecast_failure_mae.csv"), rows[1:], rows[0])

def forecast_failure_simple():
    """Simpler: MAE by weather and by partner from full df predictions."""
    PA = _mod01.PredictiveAnalytics
    pa = PA()
    pa.load_data()
    pa.prepare_features()
    pa.train_models()
    X = pa.prepare_features()
    y = pa.df["on_time_pct"].values
    pred = pa.on_time_model.predict(X)
    pa.df["_pred_ot"] = pred
    pa.df["_ae"] = (pa.df["_pred_ot"] - pa.df["on_time_pct"]).abs()
    by_weather = pa.df.groupby("weather_condition")["_ae"].mean()
    by_partner = pa.df.groupby("delivery_partner")["_ae"].mean()
    rows = [["label", "mae"]]
    for w in by_weather.index:
        rows.append([str(w)[:18], round(by_weather[w], 2)])
    for p in by_partner.index:
        rows.append([str(p)[:18], round(by_partner[p], 2)])
    _write_csv(os.path.join(out_dir, "forecast_failure_mae.csv"), rows[1:], rows[0])

def llm_country_heatmap():
    """Module 03: carbon (gCO2) per inference for each model x country."""
    CCI = _mod03.CarbonCostOfIntelligence
    cci = CCI()
    models = ["claude-3-haiku", "claude-3-sonnet", "claude-3-opus"]
    countries = ["norway", "france", "uk", "usa", "germany", "china", "india"]
    rows = [["model", "country", "carbon_gco2"]]
    by_model = {m: [] for m in models}
    for model in models:
        for country in countries:
            r = cci.calculate_inference_carbon(model_type=model, country=country, num_inferences=1)
            rows.append([model, country, round(r["carbon_gco2"], 4)])
            by_model[model].append((country, r["carbon_gco2"]))
    _write_csv(os.path.join(out_dir, "llm_country_carbon.csv"), rows[1:], rows[0])
    # Pivoted for grouped bar: country, haiku, sonnet, opus (carbon * 1000 = mg for readability)
    pivot = [["country", "haiku", "sonnet", "opus"]]
    for i, country in enumerate(countries):
        row = [country]
        for model in models:
            val = by_model[model][i][1] * 1000  # mg
            row.append(round(val, 2))
        pivot.append(row)
    _write_csv(os.path.join(out_dir, "llm_country_pivot.csv"), pivot[1:], pivot[0])

def pareto_nine_types():
    """Carbon vs service for all 9 package types - COMPUTED from dataset."""
    import pandas as pd
    from config.agent_mapping import get_agent_config
    from config.vehicle_emissions import calculate_transport_carbon
    
    # Load dataset
    PA = _mod01.PredictiveAnalytics
    pa = PA()
    pa.load_data()
    
    # Initialize AI carbon calculator (India grid)
    cci = CarbonCostOfIntelligence(default_country='india')
    
    # Compute average carbon per package type
    package_types = ['pharmacy', 'groceries', 'automobile parts', 'furniture', 
                     'documents', 'fragile items', 'electronics', 'clothing', 'cosmetics']
    
    # Y-position mapping for fig:frontier (1=Cosmetics, 9=Pharmacy)
    ypos_map = {
        'Cosmetics': 1,
        'Clothing': 2,
        'Furniture': 3,
        'Automobile parts': 4,
        'Fragile items': 5,
        'Documents': 6,
        'Electronics': 7,
        'Groceries': 8,
        'Pharmacy': 9
    }
    
    rows = [["package_type", "carbon_gco2", "service_pct", "ypos"]]
    
    for pkg_type in package_types:
        # Get config for this package type
        config = get_agent_config(pkg_type)
        cold_chain_mult = config.get('cold_chain_multiplier', 1.0)
        on_time_threshold = config.get('on_time_threshold', 0.85)
        service_pct = int(on_time_threshold * 100)
        
        # Filter dataset for this package type
        pkg_df = pa.df[pa.df['package_type'].str.lower() == pkg_type.lower()].copy()
        
        if len(pkg_df) == 0:
            # Fallback if package type not found
            pkg_name = pkg_type.title()
            if pkg_type == 'automobile parts':
                pkg_name = 'Automobile parts'
            elif pkg_type == 'fragile items':
                pkg_name = 'Fragile items'
            # Get y-position for this package type
            ypos = ypos_map.get(pkg_name, 0)
            rows.append([pkg_name, 350000, service_pct, ypos])
            continue
        
        # Compute average transport carbon with cold-chain multiplier
        pkg_df['transport_carbon'] = pkg_df.apply(
            lambda row: calculate_transport_carbon(
                distance_km=row['distance_km'],
                vehicle_type=row['vehicle_type'],
                cold_chain_multiplier=cold_chain_mult
            ), axis=1
        )
        avg_transport_carbon = pkg_df['transport_carbon'].mean()
        
        # Compute AI carbon: 3 LLM calls (Orchestrator + Risk + Sourcing) + ML predictions
        ai_result = cci.calculate_optimization_carbon(
            num_routes=3,
            model_type='gradient-boosting',
            country='india',
            include_llm_reasoning=True  # REQUIRED: 3 LLM calls per optimization
        )
        ai_carbon = ai_result['total_ai_carbon_gco2']
        
        # Total carbon
        total_carbon = avg_transport_carbon + ai_carbon
        
        # Format package name for output
        pkg_name = pkg_type.title()
        if pkg_type == 'automobile parts':
            pkg_name = 'Automobile parts'
        elif pkg_type == 'fragile items':
            pkg_name = 'Fragile items'
        
        # Get y-position for this package type
        ypos = ypos_map.get(pkg_name, 0)
        
        rows.append([pkg_name, int(round(total_carbon)), service_pct, ypos])
    
    _write_csv(os.path.join(out_dir, "pareto_9types.csv"), rows[1:], rows[0])

def cluster_scatter():
    """Vendor-level avg_cost vs on_time_pct with cluster label (Module 02)."""
    VS = _mod02.VendorSegmentation
    vs = VS()
    vs.load_data()
    vs.prepare_vendor_features()
    vs.cluster_vendors(n_clusters=4)
    # vendor_features has index = delivery_partner; cluster_labels is per-row in vendor_features order
    vf = vs.vendor_features
    vf["cluster"] = vs.cluster_labels
    rows = [["vendor", "avg_cost", "on_time_pct", "cluster"]]
    for partner in vf.index:
        rows.append([str(partner)[:20], round(vf.loc[partner, "avg_cost"], 1), round(vf.loc[partner, "on_time_pct"], 2), int(vf.loc[partner, "cluster"])])
    _write_csv(os.path.join(out_dir, "cluster_scatter.csv"), rows[1:], rows[0])

def casp_by_country():
    """CASP by country for pharmacy package type - COMPUTED from dataset."""
    import pandas as pd
    from config.agent_mapping import get_agent_config
    from config.vehicle_emissions import calculate_transport_carbon
    
    # Load dataset
    PA = _mod01.PredictiveAnalytics
    pa = PA()
    pa.load_data()
    
    # Get pharmacy config
    pharmacy_config = get_agent_config('pharmacy')
    cold_chain_mult = pharmacy_config.get('cold_chain_multiplier', 2.5)
    on_time_threshold = pharmacy_config.get('on_time_threshold', 0.99)
    service_pct = on_time_threshold * 100  # 99%
    
    # Filter dataset for pharmacy
    pharmacy_df = pa.df[pa.df['package_type'].str.lower() == 'pharmacy'].copy()
    
    if len(pharmacy_df) == 0:
        # Fallback if no pharmacy data
        rows = [["country", "casp_e6"]]
        for country in ["Norway", "France", "UK", "USA", "Germany", "India"]:
            rows.append([country, 1.0])
        _write_csv(os.path.join(out_dir, "casp_by_country.csv"), rows[1:], rows[0])
        return
    
    # Compute average transport carbon for pharmacy
    pharmacy_df['transport_carbon'] = pharmacy_df.apply(
        lambda row: calculate_transport_carbon(
            distance_km=row['distance_km'],
            vehicle_type=row['vehicle_type'],
            cold_chain_multiplier=cold_chain_mult
        ), axis=1
    )
    avg_transport_carbon = pharmacy_df['transport_carbon'].mean()
    
    # Compute CASP for each country
    countries = [
        ("Norway", "norway"),
        ("France", "france"),
        ("UK", "uk"),
        ("USA", "usa"),
        ("Germany", "germany"),
        ("India", "india")
    ]
    
    rows = [["country", "casp_e6"]]
    
    for country_name, country_code in countries:
        # Compute AI carbon for this country: 3 LLM calls + ML predictions
        cci = CarbonCostOfIntelligence(default_country=country_code)
        ai_result = cci.calculate_optimization_carbon(
            num_routes=3,
            model_type='gradient-boosting',
            country=country_code,
            include_llm_reasoning=True  # REQUIRED: 3 LLM calls per optimization
        )
        ai_carbon = ai_result['total_ai_carbon_gco2']
        
        # Total carbon
        total_carbon = avg_transport_carbon + ai_carbon
        
        # CASP = Service Performance / Total Carbon
        # service_pct is percentage (99), need to convert to decimal (0.99) for CASP calculation
        # CASP_e6 = CASP × 10^6 = (service_performance / total_carbon) × 10^6
        service_performance = service_pct / 100  # Convert percentage to decimal (99% -> 0.99)
        casp = service_performance / total_carbon if total_carbon > 0 else 0
        casp_e6 = casp * 1e6
        
        rows.append([country_name, round(casp_e6, 2)])
    
    _write_csv(os.path.join(out_dir, "casp_by_country.csv"), rows[1:], rows[0])

def early_warning_computed():
    """Real computed early-warning values for pharmacy and clothing."""
    EWS = _mod07.EarlyWarningSystem
    ews = EWS()
    ews.load_data()
    rows = [["package_type", "indicator", "value_pct", "threshold_pct", "amplification_risk"]]
    for pkg in ["pharmacy", "clothing"]:
        ind = ews.compute_early_warning_indicators(pkg)
        sc = ind["supplier_concentration_index"]
        rows.append([pkg, "Supplier concentration", sc["value_pct"], sc["threshold_pct"], sc["amplification_risk"]])
        gc = ind["geographic_clustering"]
        rows.append([pkg, "Geographic clustering", gc["value_pct"], gc["threshold_pct"], gc["amplification_risk"]])
        cc = ind["cold_chain_fragility"]
        rows.append([pkg, "Cold-chain fragility", 0, 0, cc["amplification_risk"]])
    _write_csv(os.path.join(out_dir, "early_warning_computed.csv"), rows[1:], rows[0])

def confusion_matrix_csv():
    """Write confusion matrix (Module 07) to CSV for paper figure. Uses model_evaluation_results.json if present."""
    p = os.path.join(script_dir, "model_evaluation_results.json")
    if not os.path.isfile(p):
        # Run Module 07 and get TN, FP, FN, TP from a single split
        EWS = _mod07.EarlyWarningSystem
        ews = EWS()
        ews.load_data()
        ews.train_delay_predictor()
        from sklearn.model_selection import train_test_split
        drop_cols = ["delivery_id", "delivery_time_hours", "expected_time_hours", "delayed"]
        X_df = ews.df.drop(columns=drop_cols + ["is_delayed", "delivery_status"])
        X = ews.preprocessor.transform(X_df)
        y = ews.df["is_delayed"].values
        _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        y_pred = ews.delay_model.predict(X_test)
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test, y_pred)
        tn, fp, fn, tp = cm.ravel()
    else:
        with open(p) as f:
            data = json.load(f)
        m = data.get("module_07_early_warning", {}).get("confusion_matrix", {})
        tn, fp, fn, tp = m.get("true_negatives", 0), m.get("false_positives", 0), m.get("false_negatives", 0), m.get("true_positives", 0)
    # Rows: actual, predicted, count (for TikZ/table)
    rows = [
        ["actual", "predicted", "count"],
        ["On-Time", "On-Time", tn],
        ["On-Time", "Delayed", fp],
        ["Delayed", "On-Time", fn],
        ["Delayed", "Delayed", tp],
    ]
    _write_csv(os.path.join(out_dir, "confusion_matrix.csv"), rows[1:], rows[0])

def delay_rate_by_package_type():
    """Delay rate (fraction delayed) per package type from Module 07 data."""
    EWS = _mod07.EarlyWarningSystem
    ews = EWS()
    ews.load_data()
    grp = ews.df.groupby("package_type")["is_delayed"].agg(["mean", "sum", "count"])
    grp = grp.rename(columns={"mean": "delay_rate", "sum": "n_delayed", "count": "n_total"})
    rows = [["package_type", "delay_rate", "n_delayed", "n_total"]]
    for pkg in grp.index:
        r = grp.loc[pkg]
        rows.append([str(pkg), round(float(r["delay_rate"]), 4), int(r["n_delayed"]), int(r["n_total"])])
    _write_csv(os.path.join(out_dir, "delay_rate_by_package_type.csv"), rows[1:], rows[0])

def main():
    print("Generating figure data in", out_dir)
    feature_importance_delay()
    print("  feature_importance_delay.csv")
    feature_importance_ontime()
    print("  feature_importance_ontime.csv")
    forecast_failure_mae()  # Use test data, not training data
    print("  forecast_failure_mae.csv")
    llm_country_heatmap()
    print("  llm_country_carbon.csv, llm_country_pivot.csv")
    pareto_nine_types()
    print("  pareto_9types.csv")
    cluster_scatter()
    print("  cluster_scatter.csv")
    casp_by_country()
    print("  casp_by_country.csv")
    early_warning_computed()
    print("  early_warning_computed.csv")
    confusion_matrix_csv()
    print("  confusion_matrix.csv")
    delay_rate_by_package_type()
    print("  delay_rate_by_package_type.csv")
    print("Done.")

if __name__ == "__main__":
    main()
