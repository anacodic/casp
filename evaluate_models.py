"""
Script to evaluate all ML models and collect metrics for the paper.
This will generate real R¬≤, MAE, F1, precision, recall, silhouette scores, etc.
"""

import pandas as pd
import numpy as np
import time
from sklearn.metrics import (
    r2_score, mean_absolute_error,
    classification_report, confusion_matrix,
    silhouette_score, precision_score, recall_score, f1_score,
)
from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, KFold
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
import json
import sys
import os

# Add code directory to path
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, script_dir)

# Module names starting with digits are invalid in "from X import Y"; load via importlib.
import importlib
_mod01 = importlib.import_module("modules.01_predictive_analytics")
PredictiveAnalytics = _mod01.PredictiveAnalytics
_mod02 = importlib.import_module("modules.02_vendor_segmentation")
VendorSegmentation = _mod02.VendorSegmentation
_mod07 = importlib.import_module("modules.07_early_warning_system")
EarlyWarningSystem = _mod07.EarlyWarningSystem

def evaluate_module_01():
    """Evaluate Module 01: Predictive Analytics"""
    print("\n" + "="*60)
    print("MODULE 01: PREDICTIVE ANALYTICS")
    print("="*60)
    
    analytics = PredictiveAnalytics()
    analytics.load_data()
    
    start_time = time.time()
    analytics.train_models()
    training_time = time.time() - start_time
    
    # Get test set predictions for detailed metrics
    X_processed = analytics.prepare_features()
    y_cost = analytics.df['delivery_cost'].values
    y_carbon = analytics.df['carbon_gco2'].values
    y_on_time = analytics.df['on_time_pct'].values

    X_train, X_test, y_cost_train, y_cost_test = train_test_split(
        X_processed, y_cost, test_size=0.2, random_state=42
    )
    _, _, y_carbon_train, y_carbon_test = train_test_split(
        X_processed, y_carbon, test_size=0.2, random_state=42
    )
    _, _, y_on_time_train, y_on_time_test = train_test_split(
        X_processed, y_on_time, test_size=0.2, random_state=42
    )
    
    # Get predictions
    cost_pred = analytics.cost_model.predict(X_test)
    carbon_pred = analytics.carbon_model.predict(X_test)
    on_time_pred = analytics.on_time_model.predict(X_test)
    
    # Calculate metrics
    cost_r2 = r2_score(y_cost_test, cost_pred)
    cost_mae = mean_absolute_error(y_cost_test, cost_pred)
    
    carbon_r2 = r2_score(y_carbon_test, carbon_pred)
    carbon_mae = mean_absolute_error(y_carbon_test, carbon_pred)
    
    on_time_r2 = r2_score(y_on_time_test, on_time_pred)
    on_time_mae = mean_absolute_error(y_on_time_test, on_time_pred)
    
    # Feature importance (top 5 for each model)
    cost_importance = analytics.cost_model.feature_importances_
    carbon_importance = analytics.carbon_model.feature_importances_
    on_time_importance = analytics.on_time_model.feature_importances_
    
    # Get feature names (approximate - from preprocessing)
    feature_names = analytics.feature_names if hasattr(analytics, 'feature_names') else []
    
    # 5-fold cross-validation for on-time model (and cost, for completeness)
    cat_features = ['delivery_partner', 'package_type', 'vehicle_type', 'delivery_mode', 'region', 'weather_condition']
    num_features = ['distance_km', 'package_weight_kg', 'delivery_rating']
    preprocessor_m01 = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)
        ]
    )
    X_m01 = analytics.df.drop(columns=['delivery_id', 'delivery_time_hours', 'expected_time_hours', 'delayed', 'delivery_status', 'delivery_cost', 'carbon_gco2', 'on_time_pct'])
    X_m01 = X_m01[cat_features + num_features]
    y_ot = analytics.df['on_time_pct'].values
    y_cost_cv = analytics.df['delivery_cost'].values
    pipe_ot = Pipeline([('prep', preprocessor_m01), ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))])
    pipe_cost = Pipeline([('prep', preprocessor_m01), ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))])
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_ot = cross_validate(pipe_ot, X_m01, y_ot, cv=cv, scoring=['r2', 'neg_mean_absolute_error'], return_train_score=False)
    cv_cost = cross_validate(pipe_cost, X_m01, y_cost_cv, cv=cv, scoring=['r2', 'neg_mean_absolute_error'], return_train_score=False)
    cv_on_time = {
        'r2_mean': float(np.mean(cv_ot['test_r2'])),
        'r2_std': float(np.std(cv_ot['test_r2'])),
        'mae_mean': float(np.mean(-cv_ot['test_neg_mean_absolute_error'])),
        'mae_std': float(np.std(-cv_ot['test_neg_mean_absolute_error'])),
    }
    cv_cost_cv = {
        'r2_mean': float(np.mean(cv_cost['test_r2'])),
        'r2_std': float(np.std(cv_cost['test_r2'])),
        'mae_mean': float(np.mean(-cv_cost['test_neg_mean_absolute_error'])),
        'mae_std': float(np.std(-cv_cost['test_neg_mean_absolute_error'])),
    }

    results = {
        'cost_model': {
            'r2_score': float(cost_r2),
            'mae': float(cost_mae),
            'mae_currency': f'‚Çπ{cost_mae:.2f}',
            'test_samples': len(y_cost_test),
            'cv_5fold': cv_cost_cv,
        },
        'carbon_model': {
            'r2_score': float(carbon_r2),
            'mae': float(carbon_mae),
            'mae_units': f'{carbon_mae:.0f} gCO‚ÇÇ',
            'test_samples': len(y_carbon_test)
        },
        'on_time_model': {
            'r2_score': float(on_time_r2),
            'mae': float(on_time_mae),
            'mae_units': f'{on_time_mae:.2f}%',
            'test_samples': len(y_on_time_test),
            'cv_5fold': cv_on_time,
        },
        'training_time_seconds': float(training_time),
        'dataset_size': len(analytics.df),
        'train_size': len(X_train),
        'test_size': len(X_test)
    }
    
    print(f"\n‚úì Training completed in {training_time:.2f} seconds")
    print(f"‚úì Dataset: {len(analytics.df)} records")
    print(f"‚úì 5-fold CV On-Time: R¬≤ = {cv_on_time['r2_mean']:.4f} ¬± {cv_on_time['r2_std']:.4f}, MAE = {cv_on_time['mae_mean']:.2f}% ¬± {cv_on_time['mae_std']:.2f}%")
    print(f"‚úì 5-fold CV Cost: R¬≤ = {cv_cost_cv['r2_mean']:.4f} ¬± {cv_cost_cv['r2_std']:.4f}, MAE = ‚Çπ{cv_cost_cv['mae_mean']:.2f} ¬± ‚Çπ{cv_cost_cv['mae_std']:.2f}")

    return results

def evaluate_module_02():
    """Evaluate Module 02: Vendor Segmentation"""
    print("\n" + "="*60)
    print("MODULE 02: VENDOR SEGMENTATION")
    print("="*60)
    
    segmentation = VendorSegmentation()
    segmentation.load_data()
    segmentation.prepare_vendor_features()
    
    start_time = time.time()
    segmentation.cluster_vendors(n_clusters=4)
    clustering_time = time.time() - start_time
    
    # Calculate silhouette score
    features = ['avg_cost', 'delay_rate', 'avg_rating', 'on_time_pct', 'reliability_score']
    X = segmentation.vendor_features[features].values
    X_scaled = segmentation.scaler.transform(X)
    silhouette = silhouette_score(X_scaled, segmentation.cluster_labels)
    
    # Get cluster interpretation
    clusters = segmentation.interpret_clusters()
    
    cluster_summary = {}
    for cluster_id, info in clusters.items():
        cluster_summary[int(cluster_id)] = {
            'type': info['type'],
            'vendor_count': info['count'],
            'avg_cost': float(info['avg_cost']),
            'avg_on_time_pct': float(info['avg_on_time']),
            'avg_rating': float(info['avg_rating']),
            'vendors': info['vendors']
        }
    
    results = {
        'silhouette_score': float(silhouette),
        'n_clusters': 4,
        'n_vendors': len(segmentation.vendor_features),
        'clustering_time_seconds': float(clustering_time),
        'clusters': cluster_summary
    }
    
    print(f"\n‚úì Clustering completed in {clustering_time:.2f} seconds")
    print(f"‚úì Silhouette Score: {silhouette:.4f}")
    
    return results

def evaluate_module_07():
    """Evaluate Module 07: Early Warning System"""
    print("\n" + "="*60)
    print("MODULE 07: EARLY WARNING SYSTEM")
    print("="*60)
    
    ews = EarlyWarningSystem()
    ews.load_data()
    
    start_time = time.time()
    ews.train_delay_predictor()
    training_time = time.time() - start_time
    
    # Get detailed metrics
    drop_cols = ['delivery_id', 'delivery_time_hours', 'expected_time_hours', 'delayed']
    X = ews.df.drop(columns=drop_cols + ['is_delayed', 'delivery_status'])
    y = ews.df['is_delayed'].values

    X_processed = ews.preprocessor.transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_processed, y, test_size=0.2, random_state=42, stratify=y
    )

    y_pred = ews.delay_model.predict(X_test)
    y_pred_proba = ews.delay_model.predict_proba(X_test)[:, 1]

    # Calculate metrics
    precision = precision_score(y_test, y_pred, zero_division=0)
    recall = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()

    # Classification report as dict
    report_dict = classification_report(y_test, y_pred, target_names=['On-Time', 'Delayed'], output_dict=True, zero_division=0)

    # 5-fold stratified cross-validation for delay prediction
    cat_f = ['delivery_partner', 'package_type', 'vehicle_type', 'delivery_mode', 'region', 'weather_condition']
    num_f = ['distance_km', 'package_weight_kg', 'delivery_rating']
    prep_m07 = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), num_f),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_f)
        ]
    )
    X_m07 = ews.df.drop(columns=drop_cols + ['is_delayed', 'delivery_status'])
    X_m07 = X_m07[cat_f + num_f]
    pipe_m07 = Pipeline([('prep', prep_m07), ('model', GradientBoostingClassifier(n_estimators=100, random_state=42))])
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_m07 = cross_validate(pipe_m07, X_m07, y, cv=skf, scoring=['f1', 'precision', 'recall', 'accuracy'], return_train_score=False)
    cv_07 = {
        'f1_mean': float(np.mean(cv_m07['test_f1'])),
        'f1_std': float(np.std(cv_m07['test_f1'])),
        'precision_mean': float(np.mean(cv_m07['test_precision'])),
        'precision_std': float(np.std(cv_m07['test_precision'])),
        'recall_mean': float(np.mean(cv_m07['test_recall'])),
        'recall_std': float(np.std(cv_m07['test_recall'])),
        'accuracy_mean': float(np.mean(cv_m07['test_accuracy'])),
        'accuracy_std': float(np.std(cv_m07['test_accuracy'])),
    }

    results = {
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'confusion_matrix': {
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'true_positives': int(tp)
        },
        'classification_report': report_dict,
        'training_time_seconds': float(training_time),
        'dataset_size': len(ews.df),
        'test_samples': len(y_test),
        'delay_rate': float(ews.df['is_delayed'].mean()),
        'cv_5fold': cv_07,
    }
    
    print(f"\n‚úì Training completed in {training_time:.2f} seconds")
    print(f"‚úì Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
    print(f"‚úì 5-fold CV (stratified): F1 = {cv_07['f1_mean']:.4f} ¬± {cv_07['f1_std']:.4f}, "
          f"Precision = {cv_07['precision_mean']:.4f} ¬± {cv_07['precision_std']:.4f}, "
          f"Recall = {cv_07['recall_mean']:.4f} ¬± {cv_07['recall_std']:.4f}")

    return results

def main():
    """Run all evaluations and save results"""
    print("\n" + "="*60)
    print("COMPREHENSIVE MODEL EVALUATION")
    print("="*60)
    
    all_results = {
        'module_01_predictive_analytics': None,
        'module_02_vendor_segmentation': None,
        'module_07_early_warning': None
    }
    
    try:
        all_results['module_01_predictive_analytics'] = evaluate_module_01()
    except Exception as e:
        print(f"\n‚ùå Error in Module 01: {e}")
        import traceback
        traceback.print_exc()
    
    try:
        all_results['module_02_vendor_segmentation'] = evaluate_module_02()
    except Exception as e:
        print(f"\n‚ùå Error in Module 02: {e}")
        import traceback
        traceback.print_exc()
    
    try:
        all_results['module_07_early_warning'] = evaluate_module_07()
    except Exception as e:
        print(f"\n‚ùå Error in Module 07: {e}")
        import traceback
        traceback.print_exc()
    
    # Save results to JSON
    output_file = 'model_evaluation_results.json'
    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)
    
    print("\n" + "="*60)
    print("EVALUATION COMPLETE")
    print("="*60)
    print(f"\n‚úì Results saved to: {output_file}")
    
    # Print summary
    print("\nüìä SUMMARY:")
    if all_results['module_01_predictive_analytics']:
        m01 = all_results['module_01_predictive_analytics']
        print(f"\nModule 01 - Predictive Analytics:")
        print(f"  Cost Model: R¬≤={m01['cost_model']['r2_score']:.4f}, MAE={m01['cost_model']['mae_currency']}")
        print(f"  Carbon Model: R¬≤={m01['carbon_model']['r2_score']:.4f}, MAE={m01['carbon_model']['mae_units']}")
        print(f"  On-Time Model: R¬≤={m01['on_time_model']['r2_score']:.4f}, MAE={m01['on_time_model']['mae_units']}")
        print(f"  Training Time: {m01['training_time_seconds']:.2f}s")
    
    if all_results['module_02_vendor_segmentation']:
        m02 = all_results['module_02_vendor_segmentation']
        print(f"\nModule 02 - Vendor Segmentation:")
        print(f"  Silhouette Score: {m02['silhouette_score']:.4f}")
        print(f"  Clusters: {m02['n_clusters']}, Vendors: {m02['n_vendors']}")
        print(f"  Clustering Time: {m02['clustering_time_seconds']:.2f}s")
    
    if all_results['module_07_early_warning']:
        m07 = all_results['module_07_early_warning']
        print(f"\nModule 07 - Early Warning System:")
        print(f"  Precision: {m07['precision']:.4f}")
        print(f"  Recall: {m07['recall']:.4f}")
        print(f"  F1 Score: {m07['f1_score']:.4f}")
        print(f"  Training Time: {m07['training_time_seconds']:.2f}s")
    
    return all_results

if __name__ == "__main__":
    results = main()
